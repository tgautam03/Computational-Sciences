I"è.<h2 id="introduction">Introduction</h2>
<p>From Taylor Series expansion we know that</p>

\[f(x+h)=f(x)+hf^{'}(x)+\frac{h^2}{2!}f^{''}(x)+\cdots\]

<p>If \(h\) is small enough, we can ignore terms after the first order derivative so that we get</p>

\[f(x+h)\approx f(x)+hf^{'}(x)\]

<p>Rearranging the terms, we get a formal definition of calculating derivatives</p>

\[f^{'}(x)\approx \frac{f(x+h)-f(x)}{h}\]

<blockquote>
  <p>Exact derivative is defined as 
\(f^{'}(x)=\frac{d f}{d x} = \lim_{h\to0} \frac{f(x+h)-f(x)}{h}\)</p>

  <p><strong>Claim:</strong>
The above statement suggests that as \(h \to 0\) (i.e. \(h\) is really small), we can get derivative that is very close to the actual (analytic) derivative.</p>
</blockquote>

<p>Letâ€™s try the above claim and see if that holds up in practice. Consider a function \(f(x)=2sin(x)+x^2\).</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Defining Function</span>
<span class="n">f</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Real</span><span class="x">)</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">sin</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">+</span> <span class="n">x</span><span class="o">^</span><span class="mi">2</span>
</code></pre></div></div>
<blockquote>
  <p>Letâ€™s also store the analytical derivative to compare the result against.</p>
  <div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Analytical Derivative</span>
<span class="n">df</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Real</span><span class="x">)</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">cos</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span>
</code></pre></div>  </div>
</blockquote>

<p>The plot below shows the error as \(h\) is decreased.</p>

<div class="imgcap">
<img src="/assets/02_FD/01_FD_plot.png" />
<div class="thecap">The Plot of Error as h is varied. Notice how the error explodes as h gets very close to 0. The reason behind this is the way computer handles floating point numbers/operations, and this'll prevent us from getting accuracy over 8(ish) decimal places. </div>
</div>

<h3 id="reason">Reason</h3>
<p>Letâ€™s see what actually happens in detail. Suppose we choose \(h=10^{-8}\), then \(f(x + h) = 5.818594885328427\) and \(f(x) = 5.818594853651364\).</p>

<blockquote>
  <p>Notice that \(8^{th}\) digit is the first one thatâ€™s different in both, so when these two terms are subtracted we get \(0.0000000316770628\). Next when \(10^{-8}\) is divided, we get \(3.1677062750645733\).</p>
</blockquote>

<blockquote>
  <p>See how only the first few decimal places are relevant and rest are basically junk numbers adding the error. This increases as we further reduce \(h\).
For Example, if \(h=10^{-18}\)</p>

\[f(x + h) = 5.818594853651364\]

\[f(x) = 5.818594853651364\]

\[f(x + h) - f(x) = 0.0\]

\[(f(x + h) - f(x)) / h = 0.0\]

\[sol = 3.1677063269057153\]
</blockquote>

<p>So, the main problem here is due to the fact that we are trying to store the derivative in the same variable (64 bit) as the value which leads to subtraction of two very close numbers (\(f(x+h)\) and \(f(x)\)). This is known as <strong>Truncation Error</strong>.</p>

<p>What if we could store the derivative in a separate variable?</p>

<h2 id="complex-step-differentiation">Complex Step Differentiation</h2>
<p>To store the derivative in a separate variable, we can use complex numbers where the real part will define the value of the function and the imaginary part will keep the derivative. This can be written using Taylor Series as follows</p>

\[f(x+\iota h)\approx f(x)+\iota hf^{'}(x)\]

\[\iota f^{'}(x) \approx \frac{f(x+\iota h)- f(x)}{h}\]

\[f^{'}(x) \approx \text{Imag}\bigg[\frac{f(x+\iota h)- f(x)}{h}\bigg]\]

\[f^{'}(x) \approx \text{Imag}\bigg[\frac{f(x+\iota h)}{h}\bigg]\]

<p>Letâ€™s solve the same problem as above but using complex numbers now</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Defining Function</span>
<span class="n">f</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Complex</span><span class="x">)</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">sin</span><span class="x">(</span><span class="n">x</span><span class="x">)</span> <span class="o">+</span> <span class="n">x</span><span class="o">^</span><span class="mi">2</span>
</code></pre></div></div>

<p>The following function will calcuate the derivate</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Finite DIfference</span>
<span class="k">function</span><span class="nf"> complexFiniteDiff</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Complex</span><span class="x">)</span>
    <span class="n">df_dx</span> <span class="o">=</span> <span class="n">imag</span><span class="o">.</span><span class="x">(</span><span class="n">f</span><span class="x">(</span><span class="n">x</span><span class="x">))</span><span class="o">/</span><span class="n">imag</span><span class="o">.</span><span class="x">(</span><span class="n">x</span><span class="x">)</span>
    <span class="k">return</span> <span class="n">df_dx</span>
<span class="k">end</span>
</code></pre></div></div>

<p>Plotting the results along with the previous case we can see that now the error is not exploding.</p>

<div class="imgcap">
<img src="/assets/02_FD/02_Complex_FD.png" />
<div class="thecap">From the figure above we can see that not only we avoid the explosion of error, but a steep drop in the error is observed as well. </div>
</div>

<div class="imgcap">
<img src="/assets/02_FD/02b_Complex_FD.png" />
<div class="thecap">Zoomed bottom left section of the previous plot is shown here where we can see the Complex FD approaching zero.</div>
</div>

<p>Here are the different values in the intermediate steps for \(h=10^{-18}\)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>f(complex(x, h)) = 5.818594853651364 + 3.1677063269057185e(-18)im

f(complex(x, 0)) = 5.818594853651364 + 0.0im

f(complex(x, h)) - f(complex(x, 0)) = 0.0 + 3.1677063269057185e(-18)im

imag.(f(complex(x, h))) / h = 3.1677063269057153

sol = 3.1677063269057153
</code></pre></div></div>

<p>Well, it looks like weâ€™ve solved the problem. Letâ€™s try this method on a more complicated function \(f(x) = \sum_{i=1}^N x \times i\), and see the results for different values of \(N\).</p>

<p>Below are the functions that define the problem.</p>
<div class="language-julia highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Function Definition</span>
<span class="k">function</span><span class="nf"> tough</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Complex</span><span class="x">)</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">complex</span><span class="x">(</span><span class="mi">0</span><span class="x">,</span><span class="mi">0</span><span class="x">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span>
        <span class="n">f</span> <span class="o">+=</span> <span class="n">x</span><span class="o">*</span><span class="n">i</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">f</span>
<span class="k">end</span>

<span class="c">## Complex Finite Difference</span>
<span class="k">function</span><span class="nf"> toughDiff</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Real</span><span class="x">,</span> <span class="n">h</span><span class="o">::</span><span class="kt">Real</span><span class="x">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">complexFiniteDiff</span><span class="x">(</span><span class="n">tough</span><span class="x">,</span> <span class="n">complex</span><span class="x">(</span><span class="n">x</span><span class="x">,</span><span class="n">h</span><span class="x">))</span>
    <span class="k">return</span> <span class="n">df</span>
<span class="k">end</span>

<span class="c"># Analytical Solution</span>
<span class="k">function</span><span class="nf"> actualGrad</span><span class="x">(</span><span class="n">x</span><span class="o">::</span><span class="kt">Real</span><span class="x">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="o">:</span><span class="n">n</span>
        <span class="n">df</span> <span class="o">+=</span> <span class="n">i</span>
    <span class="k">end</span>
    <span class="k">return</span> <span class="n">df</span>
<span class="k">end</span>
</code></pre></div></div>

<div class="imgcap">
<img src="/assets/02_FD/03_toughFunc1.png" />
<div class="thecap">At x=2, this plot shows the difference between the analytical gradient and the finite difference version.</div>
</div>

<div class="imgcap">
<img src="/assets/02_FD/04_toughFunc2.png" />
<div class="thecap">For the sake of completeness, Central Difference (5th order) is also shown here (using FiniteDifferences.jl library for this).</div>
</div>

<p>As you can see that errors are quite high for large values of \(N\). This mainly is due to the accumulation of Floating Point errors as the loop is executed inside the function <code class="language-plaintext highlighter-rouge">tough</code>.</p>

<p>Is there a way to solve this problem as well? The answer is yes! and <strong>Automatic Differentiation</strong> is the way to go.</p>

<div class="imgcap">
<img src="/assets/02_FD/05_autoDiff.png" />
<div class="thecap">Results after applying Automatic Differentiation shows very minimal error (only floating point precision errors) and more importantly they're constant no matter how long the function chain is.</div>
</div>

<blockquote>
  <p>In the next few posts Iâ€™ll cover Automatic Differentiation and the two flavours of it (forward and reverse mode AutoDiff).</p>
</blockquote>

<h2 id="conclusion">Conclusion</h2>
<ul>
  <li>In Finite Differences, as the step size decreases, <strong>truncation error</strong> dominates.</li>
  <li><strong>Truncation Error</strong> can be eliminated using the Complex scheme for Finite Differences.</li>
  <li><strong>Roundoff error accumulation</strong> in Finite Differences is another problem which makes this method unsuitable for tasks like <strong>Deep Learning</strong> where thousands of multiplications and additions are performed in long chains.</li>
  <li>This is where <strong>Automatic Differentiation</strong> comes in handy and is successfully used in several Deep Learning libraries like <strong>pytorch</strong>.</li>
</ul>

<h2 id="references">References</h2>
<ul>
  <li>Programming Language: <a href="https://julialang.org/">Julia Programming Language</a></li>
  <li>Example Code: <a href="https://github.com/tgautam03/lucidgrad/blob/main/nb/00_FiniteDifferences.ipynb">jupyter notebook</a></li>
  <li>Awesome Youtube Lecture: <a href="https://www.youtube.com/watch?v=zHPXGBiTM5A&amp;list=PLCAl7tjCwWyGjdzOOnlbGnVNZk0kB8VSa&amp;index=13&amp;t=1312s">Forward-Mode Automatic Differentiation (AD) via High Dimensional Algebras</a></li>
</ul>
:ET